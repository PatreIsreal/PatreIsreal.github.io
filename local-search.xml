<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/02/27/autograd/"/>
    <url>/2025/02/27/autograd/</url>
    
    <content type="html"><![CDATA[<h1 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h1><p>:label:<code>sec_autograd</code></p><p>正如 :numref:<code>sec_calculus</code>中所说，求导是几乎所有深度学习优化算法的关键步骤。<br>虽然求导的计算很简单，只需要一些基本的微积分。<br>但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。</p><p>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。<br>实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph），<br>来跟踪计算是哪些数据通过哪些操作组合起来产生输出。<br>自动微分使系统能够随后反向传播梯度。<br>这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p><h2 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h2><p>作为一个演示例子，(<strong>假设我们想对函数$y&#x3D;2\mathbf{x}^{\top}\mathbf{x}$关于列向量$\mathbf{x}$求导</strong>)。<br>首先，我们创建变量<code>x</code>并为其分配一个初始值。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">from mxnet import autograd, np, npx<br>npx.set_np()<br><br>x = np.arange(4.0)<br>x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>import torch<br><br>x = torch.arange(4.0)<br>x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>import tensorflow as tf<br><br>x = tf.range(4, dtype=tf.float32)<br>x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>import warnings<br>warnings.filterwarnings(action=&#x27;ignore&#x27;)<br>import paddle<br><br>x = paddle.arange(4, dtype=&#x27;float32&#x27;)<br>x<br></code></pre></td></tr></table></figure><p>[<strong>在我们计算$y$关于$\mathbf{x}$的梯度之前，需要一个地方来存储梯度。</strong>]<br>重要的是，我们不会在每次对一个参数求导时都分配新的内存。<br>因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。<br>注意，一个标量函数关于向量$\mathbf{x}$的梯度是向量，并且与$\mathbf{x}$具有相同的形状。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python"># 通过调用attach_grad来为一个张量的梯度分配内存<br>x.attach_grad()<br># 在计算关于x的梯度后，将能够通过&#x27;grad&#x27;属性访问它，它的值被初始化为0<br>x.grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)<br>x.grad  # 默认值是None<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>x = tf.Variable(x)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>x = paddle.to_tensor(x, stop_gradient=False)<br>x.grad  # 默认值是None<br></code></pre></td></tr></table></figure><p>(<strong>现在计算$y$。</strong>)</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python"># 把代码放到autograd.record内，以建立计算图<br>with autograd.record():<br>    y = 2 * np.dot(x, x)<br>y<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>y = 2 * torch.dot(x, x)<br>y<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br># 把所有计算记录在磁带上<br>with tf.GradientTape() as t:<br>    y = 2 * tf.tensordot(x, x, axes=1)<br>y<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>y = 2 * paddle.dot(x, x)<br>y<br></code></pre></td></tr></table></figure><p><code>x</code>是一个长度为4的向量，计算<code>x</code>和<code>x</code>的点积，得到了我们赋值给<code>y</code>的标量输出。<br>接下来，[<strong>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</strong>]，并打印这些梯度。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">y.backward()<br>x.grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>y.backward()<br>x.grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>x_grad = t.gradient(y, x)<br>x_grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>y.backward()<br>x.grad<br></code></pre></td></tr></table></figure><p>函数$y&#x3D;2\mathbf{x}^{\top}\mathbf{x}$关于$\mathbf{x}$的梯度应为$4\mathbf{x}$。<br>让我们快速验证这个梯度是否计算正确。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">x.grad == 4 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>x.grad == 4 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>x_grad == 4 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>x.grad == 4 * x<br></code></pre></td></tr></table></figure><p>[<strong>现在计算<code>x</code>的另一个函数。</strong>]</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">with autograd.record():<br>    y = x.sum()<br>y.backward()<br>x.grad  # 被新计算的梯度覆盖<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值<br>x.grad.zero_()<br>y = x.sum()<br>y.backward()<br>x.grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>with tf.GradientTape() as t:<br>    y = tf.reduce_sum(x)<br>t.gradient(y, x)  # 被新计算的梯度覆盖<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br># 在默认情况下，PaddlePaddle会累积梯度，我们需要清除之前的值<br>x.clear_gradient()<br>y = paddle.sum(x)<br>y.backward()<br>x.grad<br></code></pre></td></tr></table></figure><h2 id="非标量变量的反向传播"><a href="#非标量变量的反向传播" class="headerlink" title="非标量变量的反向传播"></a>非标量变量的反向传播</h2><p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。<br>对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</p><p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[<strong>深度学习中</strong>]），<br>但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。<br>这里(<strong>，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</strong>)</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python"># 当对向量值变量y（关于x的函数）调用backward时，将通过对y中的元素求和来创建<br># 一个新的标量变量。然后计算这个标量变量相对于x的梯度<br>with autograd.record():<br>    y = x * x  # y是一个向量<br>y.backward()<br>x.grad  # 等价于y=sum(x*x)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。<br># 本例只想求偏导数的和，所以传递一个1的梯度是合适的<br>x.grad.zero_()<br>y = x * x<br># 等价于y.backward(torch.ones(len(x)))<br>y.sum().backward()<br>x.grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>with tf.GradientTape() as t:<br>    y = x * x<br>t.gradient(y, x)  # 等价于y=tf.reduce_sum(x*x)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>x.clear_gradient()<br>y = x * x<br>paddle.sum(y).backward() <br>x.grad<br></code></pre></td></tr></table></figure><h2 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h2><p>有时，我们希望[<strong>将某些计算移动到记录的计算图之外</strong>]。<br>例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。<br>想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数，<br>并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p><p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值，<br>但丢弃计算图中如何计算<code>y</code>的任何信息。<br>换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。<br>因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理，<br>而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">with autograd.record():<br>    y = x * x<br>    u = y.detach()<br>    z = u * x<br>z.backward()<br>x.grad == u<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>x.grad.zero_()<br>y = x * x<br>u = y.detach()<br>z = u * x<br><br>z.sum().backward()<br>x.grad == u<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br># 设置persistent=True来运行t.gradient多次<br>with tf.GradientTape(persistent=True) as t:<br>    y = x * x<br>    u = tf.stop_gradient(y)<br>    z = u * x<br><br>x_grad = t.gradient(z, x)<br>x_grad == u<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>x.clear_gradient()<br>y = x * x<br>u = y.detach()<br>z = u * x<br><br>paddle.sum(z).backward()<br>x.grad == u<br></code></pre></td></tr></table></figure><p>由于记录了<code>y</code>的计算结果，我们可以随后在<code>y</code>上调用反向传播，<br>得到<code>y=x*x</code>关于的<code>x</code>的导数，即<code>2*x</code>。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">y.backward()<br>x.grad == 2 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>x.grad.zero_()<br>y.sum().backward()<br>x.grad == 2 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>t.gradient(y, x) == 2 * x<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>x.clear_gradient()<br>paddle.sum(y).backward()<br>x.grad == 2 * x<br></code></pre></td></tr></table></figure><h2 id="Python控制流的梯度计算"><a href="#Python控制流的梯度计算" class="headerlink" title="Python控制流的梯度计算"></a>Python控制流的梯度计算</h2><p>使用自动微分的一个好处是：<br>[<strong>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</strong>]。<br>在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">def f(a):<br>    b = a * 2<br>    while np.linalg.norm(b) &lt; 1000:<br>        b = b * 2<br>    if b.sum() &gt; 0:<br>        c = b<br>    else:<br>        c = 100 * b<br>    return c<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>def f(a):<br>    b = a * 2<br>    while b.norm() &lt; 1000:<br>        b = b * 2<br>    if b.sum() &gt; 0:<br>        c = b<br>    else:<br>        c = 100 * b<br>    return c<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>def f(a):<br>    b = a * 2<br>    while tf.norm(b) &lt; 1000:<br>        b = b * 2<br>    if tf.reduce_sum(b) &gt; 0:<br>        c = b<br>    else:<br>        c = 100 * b<br>    return c<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>def f(a):<br>    b = a * 2<br>    while paddle.norm(b) &lt; 1000:<br>        b = b * 2<br>    if paddle.sum(b) &gt; 0:<br>        c = b<br>    else:<br>        c = 100 * b<br>    return c<br></code></pre></td></tr></table></figure><p>让我们计算梯度。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">a = np.random.normal()<br>a.attach_grad()<br>with autograd.record():<br>    d = f(a)<br>d.backward()<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>a = torch.randn(size=(), requires_grad=True)<br>d = f(a)<br>d.backward()<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>a = tf.Variable(tf.random.normal(shape=()))<br>with tf.GradientTape() as t:<br>    d = f(a)<br>d_grad = t.gradient(d, a)<br>d_grad<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>a = paddle.to_tensor(paddle.randn(shape=[1]), stop_gradient=False)<br>d = f(a)<br>d.backward()<br></code></pre></td></tr></table></figure><p>我们现在可以分析上面定义的<code>f</code>函数。<br>请注意，它在其输入<code>a</code>中是分段线性的。<br>换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>，因此可以用<code>d/a</code>验证梯度是否正确。</p><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">a.grad == d / a<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab pytorch<br>a.grad == d / a<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab tensorflow<br>d_grad == d / a<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><figcaption><span>.input&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs &#123;.python">#@tab paddle<br>a.grad == d / a<br></code></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li></ul><h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><ol><li>为什么计算二阶导数比一阶导数的开销要更大？</li><li>在运行反向传播函数之后，立即再次运行它，看看会发生什么。</li><li>在控制流的例子中，我们计算<code>d</code>关于<code>a</code>的导数，如果将变量<code>a</code>更改为随机向量或矩阵，会发生什么？</li><li>重新设计一个求控制流梯度的例子，运行并分析结果。</li><li>使$f(x)&#x3D;\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f’(x)&#x3D;\cos(x)$。</li></ol><p>:begin_tab:<code>mxnet</code><br><a href="https://discuss.d2l.ai/t/1758">Discussions</a><br>:end_tab:</p><p>:begin_tab:<code>pytorch</code><br><a href="https://discuss.d2l.ai/t/1759">Discussions</a><br>:end_tab:</p><p>:begin_tab:<code>tensorflow</code><br><a href="https://discuss.d2l.ai/t/1757">Discussions</a><br>:end_tab:</p><p>:begin_tab:<code>paddle</code><br><a href="https://discuss.d2l.ai/t/11684">Discussions</a><br>:end_tab:</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/02/11/hello-world/"/>
    <url>/2025/02/11/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
